{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前情函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy.random.sample(size=None)\n",
    "np.random.sample(size=None) 是产生随机数的函数，size 用来指定产生的维度，如果不指定，则产生一个标量，产生数据的范围为 [0, 1)，size 的参数为整数或整数型元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar value: 0.8916937281165576\n",
      "matrix value:\n",
      " [[0.21804301 0.23273453 0.1989421  0.91833431]\n",
      " [0.96301494 0.02083241 0.44543199 0.52984906]\n",
      " [0.59214349 0.20801405 0.41085002 0.01566343]\n",
      " [0.2869535  0.22004528 0.91109743 0.24408777]]\n"
     ]
    }
   ],
   "source": [
    "scalar = np.random.sample()\n",
    "print('scalar value:', scalar)\n",
    "\n",
    "matrix = np.random.sample((4, 4))\n",
    "print('matrix value:\\n', matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow.data.Dataset.from_tensor_slices\n",
    "tf.data.Dataset.from_tensor_slices((train, label)) 是将数据的特征与标签相对应，比如创建一个 6 行的矩阵，列数随便，那么就需要 6 列标签来与 6 行矩阵的每一行数据相对应，可以发现经过 tf.data.Dataset.from_tensor_slices() 后，可以发现每一行对应一个标签，每一行有三个数据，所以 data 的维度是 ((3,), (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:\n",
      " [[0.79680143 0.7589638  0.21623016]\n",
      " [0.02987241 0.83818346 0.02416548]\n",
      " [0.3339702  0.7937907  0.36895411]\n",
      " [0.93513608 0.64110223 0.149116  ]\n",
      " [0.02685498 0.32784008 0.26686279]\n",
      " [0.92430634 0.76099528 0.08407499]] \n",
      "labels:\n",
      " [[0.76455789]\n",
      " [0.37542639]\n",
      " [0.93846915]\n",
      " [0.05775337]\n",
      " [0.48292575]\n",
      " [0.32334656]]\n",
      "<TensorSliceDataset shapes: ((3,), (1,)), types: (tf.float64, tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "features, labels = (np.random.sample((6, 3)), np.random.sample((6, 1)))\n",
    " \n",
    "print('features:\\n', features, \n",
    "      '\\nlabels:\\n', labels)\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们以 cifar10 的数据为例，train一共有 50000 个数据，每一个数据是一个三通道的图像 32x32x3，train 的标签也是 50000 条数据，所以数据与标签之间一一对应，所以 32x32x3 的图像对应一个标签，data 的输出就是 (32, 32, 3), (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train length: 50000 \n",
      "x_train shape: (50000, 32, 32, 3) \n",
      "x_test length: 10000 \n",
      "x_test shape: (10000, 32, 32, 3)\n",
      "<TensorSliceDataset shapes: ((32, 32, 3), (1,)), types: (tf.uint8, tf.uint8)>\n"
     ]
    }
   ],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('x_train length:', len(x_train),\n",
    "      '\\nx_train shape:', x_train.shape,\n",
    "      '\\nx_test length:', len(x_test),\n",
    "      '\\nx_test shape:', x_test.shape)\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow dataset 中 shuffle()、repeat()、batch()、prefetch()\n",
    "repeat(count=None) 重复此数据集 count 次，如果 count 是 None 或 -1 则是无限期重复的，返回一个 Dataset 类型，为了配合输出次数，一般默认 repeat() 为空  \n",
    "  \n",
    "batch(batch_size, drop_remainder=False) 按照顺序取出 batch_size 大小数据，最后一次输出可能小于batch 如果程序指定了每次必须输入进批次的大小，那么应将drop_remainder 设置为 True 以防止产生较小的批次，默认为 False\n",
    "  \n",
    "shuffle(buffer_size, seed=None, reshuffle_each_iteration=None) 将数据打乱，数值越大，混乱程度越大，为了完全打乱，buffer_size 应等于数据集的数量，如果数据集包含 10000 个元素但 buffer_size 设置为 1000，则shuffle最初将仅打乱这前 1000 个，读取完毕后将在进行后续打乱  \n",
    "buffer_size：表示此数据集中要从中采样新数据集的元素数 int\n",
    "seed：（可选）表示用于创建分布的随机种子 int\n",
    "reshuffle_each_iteration：（可选）布尔值，如果为true，则表示每次迭代数据集时都应进行伪随机重排(默认为True)\n",
    "返回值：  \n",
    "Dataset：A Dataset\n",
    "  \n",
    "prefetch(buffer_size) 创建一个Dataset从该数据集中预提取元素的，注意：examples.prefetch(2) 将预取2个元素（2个示例），而examples.batch(20).prefetch(2) 将预取2个元素（2个批次，每个20个示例），buffer_size 表示预取时将缓冲的最大元素数，返回 Dataset\n",
    "  \n",
    "**repeat()不进行代码展示，因为不能可视化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = (np.random.sample((10, 3)), np.random.sample((10, 1)))\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices((features, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:\n",
      " tf.Tensor(\n",
      "[[0.55558959 0.51862803 0.79967034]\n",
      " [0.63969568 0.97605433 0.94048536]\n",
      " [0.1573988  0.02895822 0.40891006]\n",
      " [0.24109004 0.35168058 0.4326879 ]], shape=(4, 3), dtype=float64) \n",
      "labels:\n",
      " tf.Tensor(\n",
      "[[0.67783353]\n",
      " [0.96823252]\n",
      " [0.47094708]\n",
      " [0.09315025]], shape=(4, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "data_batch = data.batch(4)\n",
    "for step, (batch_x, batch_y) in enumerate(data_batch.take(1), 1):\n",
    "    print('features:\\n', batch_x, '\\nlabels:\\n', batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tf.Tensor([0.63969568 0.97605433 0.94048536], shape=(3,), dtype=float64) labels: tf.Tensor([0.96823252], shape=(1,), dtype=float64)\n",
      "features: tf.Tensor([0.1573988  0.02895822 0.40891006], shape=(3,), dtype=float64) labels: tf.Tensor([0.47094708], shape=(1,), dtype=float64)\n",
      "features: tf.Tensor([0.55558959 0.51862803 0.79967034], shape=(3,), dtype=float64) labels: tf.Tensor([0.67783353], shape=(1,), dtype=float64)\n",
      "features: tf.Tensor([0.24109004 0.35168058 0.4326879 ], shape=(3,), dtype=float64) labels: tf.Tensor([0.09315025], shape=(1,), dtype=float64)\n",
      "******************************************************************line******************************************************************\n",
      "features: tf.Tensor([0.55558959 0.51862803 0.79967034], shape=(3,), dtype=float64) labels: tf.Tensor([0.67783353], shape=(1,), dtype=float64)\n",
      "features: tf.Tensor([0.63969568 0.97605433 0.94048536], shape=(3,), dtype=float64) labels: tf.Tensor([0.96823252], shape=(1,), dtype=float64)\n",
      "features: tf.Tensor([0.1573988  0.02895822 0.40891006], shape=(3,), dtype=float64) labels: tf.Tensor([0.47094708], shape=(1,), dtype=float64)\n",
      "features: tf.Tensor([0.24109004 0.35168058 0.4326879 ], shape=(3,), dtype=float64) labels: tf.Tensor([0.09315025], shape=(1,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "data_shuffle = data.shuffle(2)\n",
    "for step, (batch_x, batch_y) in enumerate(data_shuffle.take(4), 1):\n",
    "    print('features:', batch_x, 'labels:', batch_y)\n",
    "\n",
    "print(('******************************************************************line'\n",
    "       '******************************************************************'))\n",
    "\n",
    "for step, (batch_x, batch_y) in enumerate(data.take(4), 1):\n",
    "    print('features:', batch_x, 'labels:', batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只提取两个数据，每次打出一个，一个只有一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tf.Tensor([0.55558959 0.51862803 0.79967034], shape=(3,), dtype=float64) labels: tf.Tensor([0.67783353], shape=(1,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "data_pre = data.prefetch(2)\n",
    "for step, (batch_x, batch_y) in enumerate(data_pre.take(1), 1):\n",
    "    print('features:', batch_x, 'labels:', batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只提取两个数据，每次打出一个，但是具有 batch，一个有 5 行数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:\n",
      " tf.Tensor(\n",
      "[[0.55558959 0.51862803 0.79967034]\n",
      " [0.63969568 0.97605433 0.94048536]\n",
      " [0.1573988  0.02895822 0.40891006]\n",
      " [0.24109004 0.35168058 0.4326879 ]\n",
      " [0.32295129 0.4973173  0.20489247]], shape=(5, 3), dtype=float64) \n",
      "labels:\n",
      " tf.Tensor(\n",
      "[[0.67783353]\n",
      " [0.96823252]\n",
      " [0.47094708]\n",
      " [0.09315025]\n",
      " [0.17250509]], shape=(5, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "data_bat_pre = data.batch(5).prefetch(2)\n",
    "for step, (batch_x, batch_y) in enumerate(data_bat_pre.take(1), 1):\n",
    "    print('features:\\n', batch_x, '\\nlabels:\\n', batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正式开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理与参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "train_num = len(x_train)\n",
    "num_classes = 10\n",
    "\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "training_steps = 20000\n",
    "display_step = 20\n",
    "\n",
    "conv1_filters = 32\n",
    "conv2_filters = 64\n",
    "fc1_units = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置读入的模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义卷积，池化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义全连接层参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_normal = tf.initializers.RandomNormal()\n",
    "\n",
    "weights = {\n",
    "    'wc1': tf.Variable(random_normal([3, 3, 3, conv1_filters])),\n",
    "    'wc2': tf.Variable(random_normal([3, 3, conv1_filters, conv2_filters])),\n",
    "    'wd1': tf.Variable(random_normal([4096, fc1_units])),\n",
    "    'out': tf.Variable(random_normal([fc1_units, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.zeros([conv1_filters])),\n",
    "    'bc2': tf.Variable(tf.zeros([conv2_filters])),\n",
    "    'bd1': tf.Variable(tf.zeros([fc1_units])),\n",
    "    'out': tf.Variable(tf.zeros([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义前向运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x):\n",
    "    x = tf.reshape(x, [-1, 32, 32, 3])\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    pool1 = maxpool2d(conv1, k=2)\n",
    "    conv2 = conv2d(pool1, weights['wc2'], biases['bc2'])\n",
    "    pool2 = maxpool2d(conv2, k=2)\n",
    "    flat = tf.reshape(pool2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(flat, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    \n",
    "    return tf.nn.softmax(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数与准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_pred, y_true):\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "    loss_ = tf.keras.losses.sparse_categorical_crossentropy(y_true=y_true, y_pred=y_pred)\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.reshape(tf.cast(y_true, tf.int64), [-1]))\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(x, y):\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = conv_net(x)\n",
    "        loss = cross_entropy(pred, y)\n",
    "        \n",
    "    trainable_variables = list(weights.values()) + list(biases.values())\n",
    "\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 20, loss: 2.276385, accuracy: 0.265625\n",
      "step: 40, loss: 2.242373, accuracy: 0.250000\n",
      "step: 60, loss: 2.208631, accuracy: 0.281250\n",
      "step: 80, loss: 2.170714, accuracy: 0.343750\n",
      "step: 100, loss: 2.118475, accuracy: 0.406250\n",
      "step: 120, loss: 2.037787, accuracy: 0.328125\n",
      "step: 140, loss: 2.089103, accuracy: 0.281250\n",
      "step: 160, loss: 2.050472, accuracy: 0.203125\n",
      "step: 180, loss: 1.929926, accuracy: 0.234375\n",
      "step: 200, loss: 1.893998, accuracy: 0.375000\n",
      "step: 220, loss: 1.854106, accuracy: 0.328125\n",
      "step: 240, loss: 1.854870, accuracy: 0.437500\n",
      "step: 260, loss: 1.848358, accuracy: 0.390625\n",
      "step: 280, loss: 1.851551, accuracy: 0.281250\n",
      "step: 300, loss: 1.759102, accuracy: 0.437500\n",
      "step: 320, loss: 1.938205, accuracy: 0.343750\n",
      "step: 340, loss: 1.659537, accuracy: 0.390625\n",
      "step: 360, loss: 1.758072, accuracy: 0.359375\n",
      "step: 380, loss: 1.796415, accuracy: 0.390625\n",
      "step: 400, loss: 1.700016, accuracy: 0.390625\n",
      "step: 420, loss: 1.553300, accuracy: 0.421875\n",
      "step: 440, loss: 1.748674, accuracy: 0.359375\n",
      "step: 460, loss: 1.752218, accuracy: 0.328125\n",
      "step: 480, loss: 1.685862, accuracy: 0.468750\n",
      "step: 500, loss: 1.553198, accuracy: 0.468750\n",
      "step: 520, loss: 1.693633, accuracy: 0.406250\n",
      "step: 540, loss: 1.785389, accuracy: 0.375000\n",
      "step: 560, loss: 1.643481, accuracy: 0.406250\n",
      "step: 580, loss: 1.669298, accuracy: 0.406250\n",
      "step: 600, loss: 1.519601, accuracy: 0.437500\n",
      "step: 620, loss: 1.553338, accuracy: 0.500000\n",
      "step: 640, loss: 1.671875, accuracy: 0.468750\n",
      "step: 660, loss: 1.480165, accuracy: 0.546875\n",
      "step: 680, loss: 1.564311, accuracy: 0.453125\n",
      "step: 700, loss: 1.511682, accuracy: 0.343750\n",
      "step: 720, loss: 1.587633, accuracy: 0.453125\n",
      "step: 740, loss: 1.725704, accuracy: 0.343750\n",
      "step: 760, loss: 1.521984, accuracy: 0.468750\n",
      "step: 780, loss: 1.589277, accuracy: 0.468750\n",
      "step: 800, loss: 1.445964, accuracy: 0.531250\n",
      "step: 820, loss: 1.317314, accuracy: 0.593750\n",
      "step: 840, loss: 1.661648, accuracy: 0.281250\n",
      "step: 860, loss: 1.536414, accuracy: 0.421875\n",
      "step: 880, loss: 1.543383, accuracy: 0.390625\n",
      "step: 900, loss: 1.761212, accuracy: 0.296875\n",
      "step: 920, loss: 1.464802, accuracy: 0.437500\n",
      "step: 940, loss: 1.555594, accuracy: 0.515625\n",
      "step: 960, loss: 1.504519, accuracy: 0.421875\n",
      "step: 980, loss: 1.465660, accuracy: 0.515625\n",
      "step: 1000, loss: 1.500455, accuracy: 0.437500\n",
      "step: 1020, loss: 1.506013, accuracy: 0.453125\n",
      "step: 1040, loss: 1.435579, accuracy: 0.453125\n",
      "step: 1060, loss: 1.480885, accuracy: 0.468750\n",
      "step: 1080, loss: 1.286374, accuracy: 0.515625\n",
      "step: 1100, loss: 1.617996, accuracy: 0.390625\n",
      "step: 1120, loss: 1.490709, accuracy: 0.515625\n",
      "step: 1140, loss: 1.336711, accuracy: 0.562500\n",
      "step: 1160, loss: 1.398803, accuracy: 0.500000\n",
      "step: 1180, loss: 1.454815, accuracy: 0.484375\n",
      "step: 1200, loss: 1.236556, accuracy: 0.562500\n",
      "step: 1220, loss: 1.458261, accuracy: 0.500000\n",
      "step: 1240, loss: 1.540238, accuracy: 0.421875\n",
      "step: 1260, loss: 1.451048, accuracy: 0.468750\n",
      "step: 1280, loss: 1.334747, accuracy: 0.531250\n",
      "step: 1300, loss: 1.341660, accuracy: 0.593750\n",
      "step: 1320, loss: 1.583456, accuracy: 0.500000\n",
      "step: 1340, loss: 1.394101, accuracy: 0.515625\n",
      "step: 1360, loss: 1.400320, accuracy: 0.468750\n",
      "step: 1380, loss: 1.361057, accuracy: 0.546875\n",
      "step: 1400, loss: 1.372395, accuracy: 0.546875\n",
      "step: 1420, loss: 1.367206, accuracy: 0.531250\n",
      "step: 1440, loss: 1.386159, accuracy: 0.531250\n",
      "step: 1460, loss: 1.557356, accuracy: 0.500000\n",
      "step: 1480, loss: 1.669740, accuracy: 0.390625\n",
      "step: 1500, loss: 1.429871, accuracy: 0.593750\n",
      "step: 1520, loss: 1.302341, accuracy: 0.515625\n",
      "step: 1540, loss: 1.275026, accuracy: 0.593750\n",
      "step: 1560, loss: 1.348862, accuracy: 0.484375\n",
      "step: 1580, loss: 1.144465, accuracy: 0.593750\n",
      "step: 1600, loss: 1.365136, accuracy: 0.500000\n",
      "step: 1620, loss: 1.273691, accuracy: 0.562500\n",
      "step: 1640, loss: 1.141273, accuracy: 0.656250\n",
      "step: 1660, loss: 1.297894, accuracy: 0.593750\n",
      "step: 1680, loss: 1.548120, accuracy: 0.484375\n",
      "step: 1700, loss: 1.292808, accuracy: 0.531250\n",
      "step: 1720, loss: 1.403134, accuracy: 0.437500\n",
      "step: 1740, loss: 1.296756, accuracy: 0.578125\n",
      "step: 1760, loss: 1.293175, accuracy: 0.484375\n",
      "step: 1780, loss: 1.116182, accuracy: 0.609375\n",
      "step: 1800, loss: 1.104723, accuracy: 0.640625\n",
      "step: 1820, loss: 1.284589, accuracy: 0.515625\n",
      "step: 1840, loss: 1.585607, accuracy: 0.437500\n",
      "step: 1860, loss: 1.355056, accuracy: 0.468750\n",
      "step: 1880, loss: 1.516276, accuracy: 0.500000\n",
      "step: 1900, loss: 1.344898, accuracy: 0.515625\n",
      "step: 1920, loss: 1.100198, accuracy: 0.609375\n",
      "step: 1940, loss: 1.425906, accuracy: 0.484375\n",
      "step: 1960, loss: 1.234012, accuracy: 0.640625\n",
      "step: 1980, loss: 1.268929, accuracy: 0.484375\n",
      "step: 2000, loss: 1.208631, accuracy: 0.562500\n",
      "step: 2020, loss: 1.436472, accuracy: 0.546875\n",
      "step: 2040, loss: 1.427918, accuracy: 0.500000\n",
      "step: 2060, loss: 1.282104, accuracy: 0.625000\n",
      "step: 2080, loss: 1.204301, accuracy: 0.625000\n",
      "step: 2100, loss: 1.356042, accuracy: 0.515625\n",
      "step: 2120, loss: 1.183813, accuracy: 0.625000\n",
      "step: 2140, loss: 1.266040, accuracy: 0.531250\n",
      "step: 2160, loss: 1.330655, accuracy: 0.578125\n",
      "step: 2180, loss: 1.526872, accuracy: 0.390625\n",
      "step: 2200, loss: 1.035713, accuracy: 0.687500\n",
      "step: 2220, loss: 1.342928, accuracy: 0.453125\n",
      "step: 2240, loss: 1.186101, accuracy: 0.531250\n",
      "step: 2260, loss: 1.181188, accuracy: 0.625000\n",
      "step: 2280, loss: 1.250085, accuracy: 0.500000\n",
      "step: 2300, loss: 1.195667, accuracy: 0.578125\n",
      "step: 2320, loss: 1.278496, accuracy: 0.578125\n",
      "step: 2340, loss: 1.355091, accuracy: 0.500000\n",
      "step: 2360, loss: 1.505406, accuracy: 0.500000\n",
      "step: 2380, loss: 1.166971, accuracy: 0.609375\n",
      "step: 2400, loss: 1.279819, accuracy: 0.531250\n",
      "step: 2420, loss: 1.171375, accuracy: 0.656250\n",
      "step: 2440, loss: 1.405672, accuracy: 0.515625\n",
      "step: 2460, loss: 1.347829, accuracy: 0.531250\n",
      "step: 2480, loss: 1.221722, accuracy: 0.578125\n",
      "step: 2500, loss: 1.247128, accuracy: 0.546875\n",
      "step: 2520, loss: 1.354966, accuracy: 0.468750\n",
      "step: 2540, loss: 1.082253, accuracy: 0.640625\n",
      "step: 2560, loss: 1.016857, accuracy: 0.671875\n",
      "step: 2580, loss: 1.361574, accuracy: 0.531250\n",
      "step: 2600, loss: 1.084683, accuracy: 0.656250\n",
      "step: 2620, loss: 1.487545, accuracy: 0.375000\n",
      "step: 2640, loss: 1.262495, accuracy: 0.562500\n",
      "step: 2660, loss: 1.132159, accuracy: 0.609375\n",
      "step: 2680, loss: 1.585970, accuracy: 0.468750\n",
      "step: 2700, loss: 1.214496, accuracy: 0.578125\n",
      "step: 2720, loss: 1.291490, accuracy: 0.531250\n",
      "step: 2740, loss: 1.068346, accuracy: 0.625000\n",
      "step: 2760, loss: 1.126554, accuracy: 0.609375\n",
      "step: 2780, loss: 1.316472, accuracy: 0.546875\n",
      "step: 2800, loss: 1.401712, accuracy: 0.421875\n",
      "step: 2820, loss: 1.065236, accuracy: 0.640625\n",
      "step: 2840, loss: 1.193493, accuracy: 0.562500\n",
      "step: 2860, loss: 1.103419, accuracy: 0.703125\n",
      "step: 2880, loss: 1.271513, accuracy: 0.578125\n",
      "step: 2900, loss: 1.113251, accuracy: 0.609375\n",
      "step: 2920, loss: 1.031660, accuracy: 0.640625\n",
      "step: 2940, loss: 1.127907, accuracy: 0.640625\n",
      "step: 2960, loss: 1.554299, accuracy: 0.390625\n",
      "step: 2980, loss: 1.385410, accuracy: 0.546875\n",
      "step: 3000, loss: 1.422963, accuracy: 0.468750\n",
      "step: 3020, loss: 1.229560, accuracy: 0.546875\n",
      "step: 3040, loss: 1.403527, accuracy: 0.484375\n",
      "step: 3060, loss: 1.279571, accuracy: 0.531250\n",
      "step: 3080, loss: 1.264239, accuracy: 0.625000\n",
      "step: 3100, loss: 1.332562, accuracy: 0.515625\n",
      "step: 3120, loss: 1.245357, accuracy: 0.515625\n",
      "step: 3140, loss: 1.198646, accuracy: 0.562500\n",
      "step: 3160, loss: 1.099136, accuracy: 0.656250\n",
      "step: 3180, loss: 1.360137, accuracy: 0.515625\n",
      "step: 3200, loss: 1.152340, accuracy: 0.593750\n",
      "step: 3220, loss: 1.146454, accuracy: 0.625000\n",
      "step: 3240, loss: 1.141446, accuracy: 0.562500\n",
      "step: 3260, loss: 1.235465, accuracy: 0.546875\n",
      "step: 3280, loss: 1.266395, accuracy: 0.515625\n",
      "step: 3300, loss: 0.935544, accuracy: 0.687500\n",
      "step: 3320, loss: 1.182963, accuracy: 0.531250\n",
      "step: 3340, loss: 1.085849, accuracy: 0.578125\n",
      "step: 3360, loss: 1.201615, accuracy: 0.546875\n",
      "step: 3380, loss: 1.266781, accuracy: 0.593750\n",
      "step: 3400, loss: 1.176212, accuracy: 0.671875\n",
      "step: 3420, loss: 1.073300, accuracy: 0.687500\n",
      "step: 3440, loss: 1.308793, accuracy: 0.484375\n",
      "step: 3460, loss: 1.204763, accuracy: 0.562500\n",
      "step: 3480, loss: 1.236210, accuracy: 0.546875\n",
      "step: 3500, loss: 1.285755, accuracy: 0.531250\n",
      "step: 3520, loss: 1.181088, accuracy: 0.484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3540, loss: 1.401286, accuracy: 0.531250\n",
      "step: 3560, loss: 1.098115, accuracy: 0.687500\n",
      "step: 3580, loss: 1.162318, accuracy: 0.593750\n",
      "step: 3600, loss: 1.186576, accuracy: 0.578125\n",
      "step: 3620, loss: 1.336187, accuracy: 0.515625\n",
      "step: 3640, loss: 1.111418, accuracy: 0.578125\n",
      "step: 3660, loss: 1.126912, accuracy: 0.562500\n",
      "step: 3680, loss: 1.107249, accuracy: 0.640625\n",
      "step: 3700, loss: 1.204506, accuracy: 0.578125\n",
      "step: 3720, loss: 1.010100, accuracy: 0.625000\n",
      "step: 3740, loss: 1.088903, accuracy: 0.609375\n",
      "step: 3760, loss: 1.195765, accuracy: 0.671875\n",
      "step: 3780, loss: 1.226634, accuracy: 0.578125\n",
      "step: 3800, loss: 1.241634, accuracy: 0.625000\n",
      "step: 3820, loss: 1.265998, accuracy: 0.593750\n",
      "step: 3840, loss: 1.185825, accuracy: 0.609375\n",
      "step: 3860, loss: 1.093392, accuracy: 0.671875\n",
      "step: 3880, loss: 1.014271, accuracy: 0.687500\n",
      "step: 3900, loss: 1.157101, accuracy: 0.625000\n",
      "step: 3920, loss: 1.161644, accuracy: 0.515625\n",
      "step: 3940, loss: 1.201814, accuracy: 0.593750\n",
      "step: 3960, loss: 1.410638, accuracy: 0.453125\n",
      "step: 3980, loss: 1.226325, accuracy: 0.546875\n",
      "step: 4000, loss: 1.295594, accuracy: 0.453125\n",
      "step: 4020, loss: 1.113851, accuracy: 0.625000\n",
      "step: 4040, loss: 1.225008, accuracy: 0.593750\n",
      "step: 4060, loss: 1.234233, accuracy: 0.578125\n",
      "step: 4080, loss: 1.091727, accuracy: 0.671875\n",
      "step: 4100, loss: 1.197945, accuracy: 0.593750\n",
      "step: 4120, loss: 0.951892, accuracy: 0.687500\n",
      "step: 4140, loss: 1.112077, accuracy: 0.578125\n",
      "step: 4160, loss: 1.197390, accuracy: 0.640625\n",
      "step: 4180, loss: 1.298897, accuracy: 0.546875\n",
      "step: 4200, loss: 1.122173, accuracy: 0.562500\n",
      "step: 4220, loss: 0.974739, accuracy: 0.703125\n",
      "step: 4240, loss: 1.127768, accuracy: 0.625000\n",
      "step: 4260, loss: 1.068960, accuracy: 0.578125\n",
      "step: 4280, loss: 0.913063, accuracy: 0.687500\n",
      "step: 4300, loss: 1.232346, accuracy: 0.562500\n",
      "step: 4320, loss: 1.141674, accuracy: 0.593750\n",
      "step: 4340, loss: 1.165026, accuracy: 0.609375\n",
      "step: 4360, loss: 1.113790, accuracy: 0.625000\n",
      "step: 4380, loss: 1.242989, accuracy: 0.484375\n",
      "step: 4400, loss: 1.241596, accuracy: 0.578125\n",
      "step: 4420, loss: 1.271240, accuracy: 0.546875\n",
      "step: 4440, loss: 1.037379, accuracy: 0.593750\n",
      "step: 4460, loss: 1.051436, accuracy: 0.671875\n",
      "step: 4480, loss: 1.006870, accuracy: 0.640625\n",
      "step: 4500, loss: 1.246778, accuracy: 0.546875\n",
      "step: 4520, loss: 1.221698, accuracy: 0.625000\n",
      "step: 4540, loss: 1.217652, accuracy: 0.656250\n",
      "step: 4560, loss: 1.256768, accuracy: 0.562500\n",
      "step: 4580, loss: 1.112870, accuracy: 0.640625\n",
      "step: 4600, loss: 1.174084, accuracy: 0.562500\n",
      "step: 4620, loss: 1.153637, accuracy: 0.593750\n",
      "step: 4640, loss: 1.126828, accuracy: 0.625000\n",
      "step: 4660, loss: 1.274530, accuracy: 0.546875\n",
      "step: 4680, loss: 1.129411, accuracy: 0.531250\n",
      "step: 4700, loss: 0.997411, accuracy: 0.656250\n",
      "step: 4720, loss: 0.972537, accuracy: 0.703125\n",
      "step: 4740, loss: 1.354686, accuracy: 0.562500\n",
      "step: 4760, loss: 1.158436, accuracy: 0.656250\n",
      "step: 4780, loss: 1.227487, accuracy: 0.546875\n",
      "step: 4800, loss: 0.990729, accuracy: 0.656250\n",
      "step: 4820, loss: 0.995383, accuracy: 0.609375\n",
      "step: 4840, loss: 1.342955, accuracy: 0.500000\n",
      "step: 4860, loss: 1.078204, accuracy: 0.578125\n",
      "step: 4880, loss: 1.167384, accuracy: 0.625000\n",
      "step: 4900, loss: 0.991873, accuracy: 0.625000\n",
      "step: 4920, loss: 1.077211, accuracy: 0.625000\n",
      "step: 4940, loss: 0.995485, accuracy: 0.718750\n",
      "step: 4960, loss: 0.955236, accuracy: 0.656250\n",
      "step: 4980, loss: 1.143335, accuracy: 0.562500\n",
      "step: 5000, loss: 1.484373, accuracy: 0.578125\n",
      "step: 5020, loss: 0.966200, accuracy: 0.640625\n",
      "step: 5040, loss: 1.240600, accuracy: 0.609375\n",
      "step: 5060, loss: 0.976066, accuracy: 0.625000\n",
      "step: 5080, loss: 1.189202, accuracy: 0.625000\n",
      "step: 5100, loss: 1.170496, accuracy: 0.609375\n",
      "step: 5120, loss: 1.384315, accuracy: 0.593750\n",
      "step: 5140, loss: 1.272531, accuracy: 0.640625\n",
      "step: 5160, loss: 0.942095, accuracy: 0.687500\n",
      "step: 5180, loss: 1.152506, accuracy: 0.578125\n",
      "step: 5200, loss: 0.824495, accuracy: 0.828125\n",
      "step: 5220, loss: 0.972174, accuracy: 0.656250\n",
      "step: 5240, loss: 1.340806, accuracy: 0.546875\n",
      "step: 5260, loss: 1.079606, accuracy: 0.640625\n",
      "step: 5280, loss: 1.105828, accuracy: 0.609375\n",
      "step: 5300, loss: 1.191212, accuracy: 0.625000\n",
      "step: 5320, loss: 1.142700, accuracy: 0.578125\n",
      "step: 5340, loss: 1.152341, accuracy: 0.578125\n",
      "step: 5360, loss: 1.263037, accuracy: 0.500000\n",
      "step: 5380, loss: 0.962542, accuracy: 0.718750\n",
      "step: 5400, loss: 1.060646, accuracy: 0.656250\n",
      "step: 5420, loss: 1.233933, accuracy: 0.562500\n",
      "step: 5440, loss: 1.090397, accuracy: 0.625000\n",
      "step: 5460, loss: 1.162360, accuracy: 0.515625\n",
      "step: 5480, loss: 1.063299, accuracy: 0.718750\n",
      "step: 5500, loss: 1.056247, accuracy: 0.687500\n",
      "step: 5520, loss: 0.968412, accuracy: 0.656250\n",
      "step: 5540, loss: 1.084829, accuracy: 0.609375\n",
      "step: 5560, loss: 1.270253, accuracy: 0.531250\n",
      "step: 5580, loss: 1.447343, accuracy: 0.484375\n",
      "step: 5600, loss: 0.876417, accuracy: 0.703125\n",
      "step: 5620, loss: 1.043648, accuracy: 0.578125\n",
      "step: 5640, loss: 0.889093, accuracy: 0.656250\n",
      "step: 5660, loss: 1.271207, accuracy: 0.531250\n",
      "step: 5680, loss: 1.324227, accuracy: 0.640625\n",
      "step: 5700, loss: 1.120893, accuracy: 0.500000\n",
      "step: 5720, loss: 1.078401, accuracy: 0.593750\n",
      "step: 5740, loss: 1.254164, accuracy: 0.546875\n",
      "step: 5760, loss: 0.862858, accuracy: 0.687500\n",
      "step: 5780, loss: 1.215546, accuracy: 0.593750\n",
      "step: 5800, loss: 0.996863, accuracy: 0.687500\n",
      "step: 5820, loss: 1.017749, accuracy: 0.609375\n",
      "step: 5840, loss: 1.040961, accuracy: 0.640625\n",
      "step: 5860, loss: 0.983268, accuracy: 0.656250\n",
      "step: 5880, loss: 0.849505, accuracy: 0.718750\n",
      "step: 5900, loss: 1.068814, accuracy: 0.625000\n",
      "step: 5920, loss: 1.121142, accuracy: 0.609375\n",
      "step: 5940, loss: 0.991480, accuracy: 0.671875\n",
      "step: 5960, loss: 0.877547, accuracy: 0.734375\n",
      "step: 5980, loss: 1.115197, accuracy: 0.609375\n",
      "step: 6000, loss: 1.056266, accuracy: 0.625000\n",
      "step: 6020, loss: 1.093345, accuracy: 0.609375\n",
      "step: 6040, loss: 1.115960, accuracy: 0.593750\n",
      "step: 6060, loss: 1.074335, accuracy: 0.609375\n",
      "step: 6080, loss: 1.079632, accuracy: 0.640625\n",
      "step: 6100, loss: 1.104883, accuracy: 0.625000\n",
      "step: 6120, loss: 0.850795, accuracy: 0.687500\n",
      "step: 6140, loss: 1.109714, accuracy: 0.625000\n",
      "step: 6160, loss: 1.236627, accuracy: 0.593750\n",
      "step: 6180, loss: 1.075707, accuracy: 0.578125\n",
      "step: 6200, loss: 1.100365, accuracy: 0.671875\n",
      "step: 6220, loss: 0.939263, accuracy: 0.687500\n",
      "step: 6240, loss: 0.925581, accuracy: 0.687500\n",
      "step: 6260, loss: 1.209648, accuracy: 0.593750\n",
      "step: 6280, loss: 0.907607, accuracy: 0.671875\n",
      "step: 6300, loss: 1.072420, accuracy: 0.640625\n",
      "step: 6320, loss: 0.806191, accuracy: 0.765625\n",
      "step: 6340, loss: 0.969761, accuracy: 0.578125\n",
      "step: 6360, loss: 1.272927, accuracy: 0.515625\n",
      "step: 6380, loss: 1.098439, accuracy: 0.640625\n",
      "step: 6400, loss: 0.970890, accuracy: 0.609375\n",
      "step: 6420, loss: 0.946843, accuracy: 0.609375\n",
      "step: 6440, loss: 0.917769, accuracy: 0.687500\n",
      "step: 6460, loss: 1.270291, accuracy: 0.531250\n",
      "step: 6480, loss: 1.030740, accuracy: 0.625000\n",
      "step: 6500, loss: 0.998302, accuracy: 0.625000\n",
      "step: 6520, loss: 1.149911, accuracy: 0.625000\n",
      "step: 6540, loss: 1.093208, accuracy: 0.531250\n",
      "step: 6560, loss: 0.947087, accuracy: 0.562500\n",
      "step: 6580, loss: 1.134180, accuracy: 0.703125\n",
      "step: 6600, loss: 1.041888, accuracy: 0.546875\n",
      "step: 6620, loss: 0.986025, accuracy: 0.625000\n",
      "step: 6640, loss: 1.206340, accuracy: 0.578125\n",
      "step: 6660, loss: 0.859850, accuracy: 0.640625\n",
      "step: 6680, loss: 1.088959, accuracy: 0.640625\n",
      "step: 6700, loss: 0.962001, accuracy: 0.656250\n",
      "step: 6720, loss: 1.039606, accuracy: 0.656250\n",
      "step: 6740, loss: 0.867526, accuracy: 0.703125\n",
      "step: 6760, loss: 1.044495, accuracy: 0.640625\n",
      "step: 6780, loss: 1.054794, accuracy: 0.609375\n",
      "step: 6800, loss: 1.058500, accuracy: 0.625000\n",
      "step: 6820, loss: 0.970515, accuracy: 0.656250\n",
      "step: 6840, loss: 0.942591, accuracy: 0.703125\n",
      "step: 6860, loss: 0.818612, accuracy: 0.734375\n",
      "step: 6880, loss: 0.962604, accuracy: 0.656250\n",
      "step: 6900, loss: 1.267661, accuracy: 0.484375\n",
      "step: 6920, loss: 1.298820, accuracy: 0.562500\n",
      "step: 6940, loss: 0.934287, accuracy: 0.687500\n",
      "step: 6960, loss: 0.759941, accuracy: 0.750000\n",
      "step: 6980, loss: 1.134099, accuracy: 0.640625\n",
      "step: 7000, loss: 0.976294, accuracy: 0.609375\n",
      "step: 7020, loss: 1.081850, accuracy: 0.625000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7040, loss: 0.967251, accuracy: 0.718750\n",
      "step: 7060, loss: 1.087622, accuracy: 0.562500\n",
      "step: 7080, loss: 0.984223, accuracy: 0.625000\n",
      "step: 7100, loss: 1.071772, accuracy: 0.656250\n",
      "step: 7120, loss: 0.815130, accuracy: 0.718750\n",
      "step: 7140, loss: 0.974542, accuracy: 0.656250\n",
      "step: 7160, loss: 0.775125, accuracy: 0.718750\n",
      "step: 7180, loss: 0.957779, accuracy: 0.703125\n",
      "step: 7200, loss: 0.734328, accuracy: 0.734375\n",
      "step: 7220, loss: 1.018757, accuracy: 0.671875\n",
      "step: 7240, loss: 0.946163, accuracy: 0.703125\n",
      "step: 7260, loss: 0.722667, accuracy: 0.796875\n",
      "step: 7280, loss: 1.087558, accuracy: 0.656250\n",
      "step: 7300, loss: 0.847729, accuracy: 0.718750\n",
      "step: 7320, loss: 1.002224, accuracy: 0.640625\n",
      "step: 7340, loss: 1.067971, accuracy: 0.671875\n",
      "step: 7360, loss: 0.887718, accuracy: 0.671875\n",
      "step: 7380, loss: 1.045760, accuracy: 0.562500\n",
      "step: 7400, loss: 0.887065, accuracy: 0.671875\n",
      "step: 7420, loss: 0.859414, accuracy: 0.765625\n",
      "step: 7440, loss: 1.007464, accuracy: 0.625000\n",
      "step: 7460, loss: 0.991326, accuracy: 0.609375\n",
      "step: 7480, loss: 1.050506, accuracy: 0.718750\n",
      "step: 7500, loss: 0.703598, accuracy: 0.765625\n",
      "step: 7520, loss: 1.124963, accuracy: 0.625000\n",
      "step: 7540, loss: 1.029235, accuracy: 0.640625\n",
      "step: 7560, loss: 0.840012, accuracy: 0.671875\n",
      "step: 7580, loss: 1.114191, accuracy: 0.593750\n",
      "step: 7600, loss: 0.859919, accuracy: 0.687500\n",
      "step: 7620, loss: 1.046746, accuracy: 0.609375\n",
      "step: 7640, loss: 1.080437, accuracy: 0.593750\n",
      "step: 7660, loss: 0.936739, accuracy: 0.656250\n",
      "step: 7680, loss: 0.842475, accuracy: 0.734375\n",
      "step: 7700, loss: 0.959084, accuracy: 0.671875\n",
      "step: 7720, loss: 0.922306, accuracy: 0.640625\n",
      "step: 7740, loss: 1.115422, accuracy: 0.671875\n",
      "step: 7760, loss: 0.808772, accuracy: 0.718750\n",
      "step: 7780, loss: 1.123793, accuracy: 0.593750\n",
      "step: 7800, loss: 0.976914, accuracy: 0.703125\n",
      "step: 7820, loss: 0.927014, accuracy: 0.703125\n",
      "step: 7840, loss: 1.004652, accuracy: 0.640625\n",
      "step: 7860, loss: 1.004949, accuracy: 0.640625\n",
      "step: 7880, loss: 1.003322, accuracy: 0.671875\n",
      "step: 7900, loss: 1.143335, accuracy: 0.625000\n",
      "step: 7920, loss: 0.982034, accuracy: 0.609375\n",
      "step: 7940, loss: 0.975877, accuracy: 0.640625\n",
      "step: 7960, loss: 0.993036, accuracy: 0.671875\n",
      "step: 7980, loss: 1.044545, accuracy: 0.671875\n",
      "step: 8000, loss: 0.758229, accuracy: 0.781250\n",
      "step: 8020, loss: 1.081328, accuracy: 0.625000\n",
      "step: 8040, loss: 0.882493, accuracy: 0.656250\n",
      "step: 8060, loss: 0.932396, accuracy: 0.718750\n",
      "step: 8080, loss: 0.975143, accuracy: 0.640625\n",
      "step: 8100, loss: 0.914641, accuracy: 0.734375\n",
      "step: 8120, loss: 1.021448, accuracy: 0.656250\n",
      "step: 8140, loss: 1.095585, accuracy: 0.640625\n",
      "step: 8160, loss: 0.934417, accuracy: 0.687500\n",
      "step: 8180, loss: 0.856741, accuracy: 0.703125\n",
      "step: 8200, loss: 1.232145, accuracy: 0.593750\n",
      "step: 8220, loss: 1.036476, accuracy: 0.625000\n",
      "step: 8240, loss: 1.139990, accuracy: 0.546875\n",
      "step: 8260, loss: 0.931618, accuracy: 0.671875\n",
      "step: 8280, loss: 1.157766, accuracy: 0.609375\n",
      "step: 8300, loss: 0.885197, accuracy: 0.750000\n",
      "step: 8320, loss: 0.982414, accuracy: 0.734375\n",
      "step: 8340, loss: 1.000403, accuracy: 0.687500\n",
      "step: 8360, loss: 1.137939, accuracy: 0.640625\n",
      "step: 8380, loss: 1.045910, accuracy: 0.671875\n",
      "step: 8400, loss: 1.027700, accuracy: 0.640625\n",
      "step: 8420, loss: 0.780123, accuracy: 0.734375\n",
      "step: 8440, loss: 0.913437, accuracy: 0.703125\n",
      "step: 8460, loss: 1.138937, accuracy: 0.625000\n",
      "step: 8480, loss: 0.919909, accuracy: 0.656250\n",
      "step: 8500, loss: 1.067353, accuracy: 0.687500\n",
      "step: 8520, loss: 1.003885, accuracy: 0.656250\n",
      "step: 8540, loss: 0.899201, accuracy: 0.640625\n",
      "step: 8560, loss: 1.101071, accuracy: 0.578125\n",
      "step: 8580, loss: 1.080865, accuracy: 0.687500\n",
      "step: 8600, loss: 0.946482, accuracy: 0.703125\n",
      "step: 8620, loss: 0.999890, accuracy: 0.593750\n",
      "step: 8640, loss: 0.855667, accuracy: 0.671875\n",
      "step: 8660, loss: 1.007296, accuracy: 0.671875\n",
      "step: 8680, loss: 0.913402, accuracy: 0.718750\n",
      "step: 8700, loss: 0.889089, accuracy: 0.703125\n",
      "step: 8720, loss: 0.782964, accuracy: 0.765625\n",
      "step: 8740, loss: 0.713313, accuracy: 0.734375\n",
      "step: 8760, loss: 1.071276, accuracy: 0.593750\n",
      "step: 8780, loss: 0.701770, accuracy: 0.718750\n",
      "step: 8800, loss: 0.929853, accuracy: 0.718750\n",
      "step: 8820, loss: 1.134374, accuracy: 0.656250\n",
      "step: 8840, loss: 1.133070, accuracy: 0.562500\n",
      "step: 8860, loss: 0.916066, accuracy: 0.640625\n",
      "step: 8880, loss: 1.025177, accuracy: 0.640625\n",
      "step: 8900, loss: 0.735643, accuracy: 0.765625\n",
      "step: 8920, loss: 0.690560, accuracy: 0.765625\n",
      "step: 8940, loss: 1.130279, accuracy: 0.640625\n",
      "step: 8960, loss: 0.886598, accuracy: 0.750000\n",
      "step: 8980, loss: 0.985096, accuracy: 0.703125\n",
      "step: 9000, loss: 0.869804, accuracy: 0.703125\n",
      "step: 9020, loss: 0.788837, accuracy: 0.718750\n",
      "step: 9040, loss: 0.825027, accuracy: 0.750000\n",
      "step: 9060, loss: 0.721839, accuracy: 0.796875\n",
      "step: 9080, loss: 0.751495, accuracy: 0.687500\n",
      "step: 9100, loss: 1.011162, accuracy: 0.703125\n",
      "step: 9120, loss: 0.929641, accuracy: 0.703125\n",
      "step: 9140, loss: 1.101192, accuracy: 0.671875\n",
      "step: 9160, loss: 1.080045, accuracy: 0.703125\n",
      "step: 9180, loss: 0.910075, accuracy: 0.687500\n",
      "step: 9200, loss: 1.019661, accuracy: 0.640625\n",
      "step: 9220, loss: 1.135468, accuracy: 0.640625\n",
      "step: 9240, loss: 0.857590, accuracy: 0.671875\n",
      "step: 9260, loss: 1.037319, accuracy: 0.625000\n",
      "step: 9280, loss: 0.726033, accuracy: 0.781250\n",
      "step: 9300, loss: 0.897751, accuracy: 0.734375\n",
      "step: 9320, loss: 1.086192, accuracy: 0.703125\n",
      "step: 9340, loss: 0.814269, accuracy: 0.750000\n",
      "step: 9360, loss: 0.965713, accuracy: 0.718750\n",
      "step: 9380, loss: 1.022307, accuracy: 0.625000\n",
      "step: 9400, loss: 0.973002, accuracy: 0.687500\n",
      "step: 9420, loss: 0.776952, accuracy: 0.718750\n",
      "step: 9440, loss: 0.978320, accuracy: 0.625000\n",
      "step: 9460, loss: 0.738735, accuracy: 0.750000\n",
      "step: 9480, loss: 0.749078, accuracy: 0.765625\n",
      "step: 9500, loss: 0.858012, accuracy: 0.703125\n",
      "step: 9520, loss: 0.895090, accuracy: 0.687500\n",
      "step: 9540, loss: 0.888939, accuracy: 0.656250\n",
      "step: 9560, loss: 1.006202, accuracy: 0.593750\n",
      "step: 9580, loss: 1.350507, accuracy: 0.500000\n",
      "step: 9600, loss: 1.059741, accuracy: 0.656250\n",
      "step: 9620, loss: 0.951142, accuracy: 0.718750\n",
      "step: 9640, loss: 1.044319, accuracy: 0.625000\n",
      "step: 9660, loss: 0.828242, accuracy: 0.765625\n",
      "step: 9680, loss: 0.875872, accuracy: 0.703125\n",
      "step: 9700, loss: 0.989715, accuracy: 0.718750\n",
      "step: 9720, loss: 1.040553, accuracy: 0.593750\n",
      "step: 9740, loss: 0.911621, accuracy: 0.734375\n",
      "step: 9760, loss: 0.874865, accuracy: 0.609375\n",
      "step: 9780, loss: 0.741037, accuracy: 0.718750\n",
      "step: 9800, loss: 1.133057, accuracy: 0.640625\n",
      "step: 9820, loss: 0.897769, accuracy: 0.734375\n",
      "step: 9840, loss: 0.814037, accuracy: 0.750000\n",
      "step: 9860, loss: 1.195446, accuracy: 0.562500\n",
      "step: 9880, loss: 0.937777, accuracy: 0.609375\n",
      "step: 9900, loss: 0.810848, accuracy: 0.687500\n",
      "step: 9920, loss: 0.723614, accuracy: 0.734375\n",
      "step: 9940, loss: 0.856819, accuracy: 0.718750\n",
      "step: 9960, loss: 0.833206, accuracy: 0.734375\n",
      "step: 9980, loss: 0.902507, accuracy: 0.625000\n",
      "step: 10000, loss: 0.887030, accuracy: 0.734375\n",
      "step: 10020, loss: 0.615975, accuracy: 0.812500\n",
      "step: 10040, loss: 0.900393, accuracy: 0.687500\n",
      "step: 10060, loss: 1.122524, accuracy: 0.656250\n",
      "step: 10080, loss: 0.913300, accuracy: 0.640625\n",
      "step: 10100, loss: 0.829970, accuracy: 0.671875\n",
      "step: 10120, loss: 1.095160, accuracy: 0.656250\n",
      "step: 10140, loss: 0.571449, accuracy: 0.859375\n",
      "step: 10160, loss: 0.831842, accuracy: 0.718750\n",
      "step: 10180, loss: 0.925483, accuracy: 0.687500\n",
      "step: 10200, loss: 0.927694, accuracy: 0.718750\n",
      "step: 10220, loss: 0.967038, accuracy: 0.625000\n",
      "step: 10240, loss: 0.588016, accuracy: 0.843750\n",
      "step: 10260, loss: 0.895630, accuracy: 0.656250\n",
      "step: 10280, loss: 0.966838, accuracy: 0.703125\n",
      "step: 10300, loss: 0.834982, accuracy: 0.671875\n",
      "step: 10320, loss: 1.014441, accuracy: 0.656250\n",
      "step: 10340, loss: 0.892982, accuracy: 0.703125\n",
      "step: 10360, loss: 0.987956, accuracy: 0.640625\n",
      "step: 10380, loss: 0.780270, accuracy: 0.765625\n",
      "step: 10400, loss: 0.974033, accuracy: 0.625000\n",
      "step: 10420, loss: 0.807693, accuracy: 0.734375\n",
      "step: 10440, loss: 0.768007, accuracy: 0.734375\n",
      "step: 10460, loss: 1.034807, accuracy: 0.671875\n",
      "step: 10480, loss: 0.841930, accuracy: 0.734375\n",
      "step: 10500, loss: 0.836936, accuracy: 0.703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10520, loss: 0.794537, accuracy: 0.765625\n",
      "step: 10540, loss: 0.823349, accuracy: 0.718750\n",
      "step: 10560, loss: 0.720228, accuracy: 0.812500\n",
      "step: 10580, loss: 0.880850, accuracy: 0.671875\n",
      "step: 10600, loss: 0.807551, accuracy: 0.734375\n",
      "step: 10620, loss: 0.993610, accuracy: 0.687500\n",
      "step: 10640, loss: 0.813218, accuracy: 0.703125\n",
      "step: 10660, loss: 0.866734, accuracy: 0.687500\n",
      "step: 10680, loss: 0.865751, accuracy: 0.750000\n",
      "step: 10700, loss: 0.905100, accuracy: 0.671875\n",
      "step: 10720, loss: 0.816091, accuracy: 0.703125\n",
      "step: 10740, loss: 0.927788, accuracy: 0.703125\n",
      "step: 10760, loss: 1.134274, accuracy: 0.578125\n",
      "step: 10780, loss: 1.016891, accuracy: 0.609375\n",
      "step: 10800, loss: 0.867915, accuracy: 0.750000\n",
      "step: 10820, loss: 1.045817, accuracy: 0.656250\n",
      "step: 10840, loss: 0.735716, accuracy: 0.796875\n",
      "step: 10860, loss: 0.872739, accuracy: 0.640625\n",
      "step: 10880, loss: 0.893893, accuracy: 0.640625\n",
      "step: 10900, loss: 0.729596, accuracy: 0.828125\n",
      "step: 10920, loss: 0.917841, accuracy: 0.718750\n",
      "step: 10940, loss: 1.027666, accuracy: 0.656250\n",
      "step: 10960, loss: 0.979721, accuracy: 0.703125\n",
      "step: 10980, loss: 0.708285, accuracy: 0.812500\n",
      "step: 11000, loss: 0.797626, accuracy: 0.703125\n",
      "step: 11020, loss: 0.639345, accuracy: 0.765625\n",
      "step: 11040, loss: 1.026170, accuracy: 0.640625\n",
      "step: 11060, loss: 0.968848, accuracy: 0.656250\n",
      "step: 11080, loss: 0.594537, accuracy: 0.812500\n",
      "step: 11100, loss: 0.966551, accuracy: 0.609375\n",
      "step: 11120, loss: 1.032480, accuracy: 0.640625\n",
      "step: 11140, loss: 0.958942, accuracy: 0.609375\n",
      "step: 11160, loss: 0.865774, accuracy: 0.625000\n",
      "step: 11180, loss: 0.896442, accuracy: 0.765625\n",
      "step: 11200, loss: 0.688561, accuracy: 0.796875\n",
      "step: 11220, loss: 0.795556, accuracy: 0.656250\n",
      "step: 11240, loss: 0.786636, accuracy: 0.734375\n",
      "step: 11260, loss: 0.892902, accuracy: 0.687500\n",
      "step: 11280, loss: 0.920960, accuracy: 0.609375\n",
      "step: 11300, loss: 0.949273, accuracy: 0.656250\n",
      "step: 11320, loss: 0.958793, accuracy: 0.640625\n",
      "step: 11340, loss: 0.971343, accuracy: 0.640625\n",
      "step: 11360, loss: 0.844444, accuracy: 0.734375\n",
      "step: 11380, loss: 0.769562, accuracy: 0.718750\n",
      "step: 11400, loss: 0.835767, accuracy: 0.703125\n",
      "step: 11420, loss: 1.029431, accuracy: 0.687500\n",
      "step: 11440, loss: 0.953223, accuracy: 0.640625\n",
      "step: 11460, loss: 0.941630, accuracy: 0.703125\n",
      "step: 11480, loss: 1.159796, accuracy: 0.625000\n",
      "step: 11500, loss: 0.793590, accuracy: 0.765625\n",
      "step: 11520, loss: 1.054829, accuracy: 0.625000\n",
      "step: 11540, loss: 0.819774, accuracy: 0.718750\n",
      "step: 11560, loss: 0.809488, accuracy: 0.734375\n",
      "step: 11580, loss: 1.059028, accuracy: 0.625000\n",
      "step: 11600, loss: 0.852282, accuracy: 0.750000\n",
      "step: 11620, loss: 0.845210, accuracy: 0.687500\n",
      "step: 11640, loss: 0.819882, accuracy: 0.734375\n",
      "step: 11660, loss: 0.801157, accuracy: 0.718750\n",
      "step: 11680, loss: 0.898654, accuracy: 0.703125\n",
      "step: 11700, loss: 0.572162, accuracy: 0.859375\n",
      "step: 11720, loss: 0.864487, accuracy: 0.703125\n",
      "step: 11740, loss: 1.018308, accuracy: 0.640625\n",
      "step: 11760, loss: 0.979032, accuracy: 0.750000\n",
      "step: 11780, loss: 0.796056, accuracy: 0.765625\n",
      "step: 11800, loss: 0.756393, accuracy: 0.750000\n",
      "step: 11820, loss: 0.792663, accuracy: 0.703125\n",
      "step: 11840, loss: 0.755899, accuracy: 0.687500\n",
      "step: 11860, loss: 0.869337, accuracy: 0.687500\n",
      "step: 11880, loss: 1.005254, accuracy: 0.625000\n",
      "step: 11900, loss: 0.821619, accuracy: 0.718750\n",
      "step: 11920, loss: 0.752356, accuracy: 0.703125\n",
      "step: 11940, loss: 0.967064, accuracy: 0.640625\n",
      "step: 11960, loss: 0.992434, accuracy: 0.671875\n",
      "step: 11980, loss: 0.802480, accuracy: 0.703125\n",
      "step: 12000, loss: 0.798196, accuracy: 0.734375\n",
      "step: 12020, loss: 0.779082, accuracy: 0.718750\n",
      "step: 12040, loss: 0.656412, accuracy: 0.781250\n",
      "step: 12060, loss: 0.811073, accuracy: 0.687500\n",
      "step: 12080, loss: 0.818470, accuracy: 0.671875\n",
      "step: 12100, loss: 0.722963, accuracy: 0.765625\n",
      "step: 12120, loss: 0.774058, accuracy: 0.796875\n",
      "step: 12140, loss: 0.835853, accuracy: 0.671875\n",
      "step: 12160, loss: 0.714955, accuracy: 0.781250\n",
      "step: 12180, loss: 0.810870, accuracy: 0.703125\n",
      "step: 12200, loss: 0.805659, accuracy: 0.734375\n",
      "step: 12220, loss: 0.852562, accuracy: 0.687500\n",
      "step: 12240, loss: 0.816268, accuracy: 0.687500\n",
      "step: 12260, loss: 0.963522, accuracy: 0.625000\n",
      "step: 12280, loss: 0.966990, accuracy: 0.640625\n",
      "step: 12300, loss: 0.726242, accuracy: 0.781250\n",
      "step: 12320, loss: 0.723178, accuracy: 0.781250\n",
      "step: 12340, loss: 0.953744, accuracy: 0.656250\n",
      "step: 12360, loss: 0.832067, accuracy: 0.656250\n",
      "step: 12380, loss: 0.821767, accuracy: 0.640625\n",
      "step: 12400, loss: 0.690452, accuracy: 0.812500\n",
      "step: 12420, loss: 0.914985, accuracy: 0.640625\n",
      "step: 12440, loss: 0.940682, accuracy: 0.671875\n",
      "step: 12460, loss: 0.772017, accuracy: 0.703125\n",
      "step: 12480, loss: 0.758194, accuracy: 0.812500\n",
      "step: 12500, loss: 1.020104, accuracy: 0.656250\n",
      "step: 12520, loss: 0.862421, accuracy: 0.703125\n",
      "step: 12540, loss: 0.818422, accuracy: 0.734375\n",
      "step: 12560, loss: 0.821446, accuracy: 0.656250\n",
      "step: 12580, loss: 0.708522, accuracy: 0.765625\n",
      "step: 12600, loss: 0.717051, accuracy: 0.734375\n",
      "step: 12620, loss: 0.663236, accuracy: 0.765625\n",
      "step: 12640, loss: 0.678479, accuracy: 0.812500\n",
      "step: 12660, loss: 0.750746, accuracy: 0.703125\n",
      "step: 12680, loss: 0.908067, accuracy: 0.718750\n",
      "step: 12700, loss: 0.948128, accuracy: 0.687500\n",
      "step: 12720, loss: 0.831045, accuracy: 0.734375\n",
      "step: 12740, loss: 0.764560, accuracy: 0.718750\n",
      "step: 12760, loss: 0.765291, accuracy: 0.671875\n",
      "step: 12780, loss: 0.683561, accuracy: 0.765625\n",
      "step: 12800, loss: 0.823146, accuracy: 0.703125\n",
      "step: 12820, loss: 0.902133, accuracy: 0.671875\n",
      "step: 12840, loss: 0.887448, accuracy: 0.656250\n",
      "step: 12860, loss: 0.704702, accuracy: 0.765625\n",
      "step: 12880, loss: 0.754489, accuracy: 0.765625\n",
      "step: 12900, loss: 0.877238, accuracy: 0.734375\n",
      "step: 12920, loss: 0.710903, accuracy: 0.796875\n",
      "step: 12940, loss: 0.840148, accuracy: 0.734375\n",
      "step: 12960, loss: 0.823443, accuracy: 0.750000\n",
      "step: 12980, loss: 0.787419, accuracy: 0.750000\n",
      "step: 13000, loss: 0.630215, accuracy: 0.750000\n",
      "step: 13020, loss: 0.646158, accuracy: 0.796875\n",
      "step: 13040, loss: 0.969781, accuracy: 0.671875\n",
      "step: 13060, loss: 0.706625, accuracy: 0.765625\n",
      "step: 13080, loss: 1.210763, accuracy: 0.609375\n",
      "step: 13100, loss: 0.853941, accuracy: 0.640625\n",
      "step: 13120, loss: 0.593151, accuracy: 0.812500\n",
      "step: 13140, loss: 0.741755, accuracy: 0.703125\n",
      "step: 13160, loss: 0.754753, accuracy: 0.750000\n",
      "step: 13180, loss: 0.898139, accuracy: 0.640625\n",
      "step: 13200, loss: 0.748034, accuracy: 0.781250\n",
      "step: 13220, loss: 0.972162, accuracy: 0.687500\n",
      "step: 13240, loss: 0.867140, accuracy: 0.687500\n",
      "step: 13260, loss: 0.680285, accuracy: 0.812500\n",
      "step: 13280, loss: 0.718613, accuracy: 0.750000\n",
      "step: 13300, loss: 0.842294, accuracy: 0.656250\n",
      "step: 13320, loss: 0.935908, accuracy: 0.671875\n",
      "step: 13340, loss: 0.713207, accuracy: 0.687500\n",
      "step: 13360, loss: 0.510060, accuracy: 0.843750\n",
      "step: 13380, loss: 0.653474, accuracy: 0.734375\n",
      "step: 13400, loss: 0.874628, accuracy: 0.687500\n",
      "step: 13420, loss: 0.909551, accuracy: 0.750000\n",
      "step: 13440, loss: 0.731380, accuracy: 0.812500\n",
      "step: 13460, loss: 0.730612, accuracy: 0.734375\n",
      "step: 13480, loss: 1.000436, accuracy: 0.640625\n",
      "step: 13500, loss: 0.650838, accuracy: 0.765625\n",
      "step: 13520, loss: 0.985456, accuracy: 0.640625\n",
      "step: 13540, loss: 0.765255, accuracy: 0.750000\n",
      "step: 13560, loss: 0.801309, accuracy: 0.750000\n",
      "step: 13580, loss: 0.976026, accuracy: 0.671875\n",
      "step: 13600, loss: 0.940072, accuracy: 0.625000\n",
      "step: 13620, loss: 0.486858, accuracy: 0.859375\n",
      "step: 13640, loss: 0.585053, accuracy: 0.796875\n",
      "step: 13660, loss: 0.845228, accuracy: 0.734375\n",
      "step: 13680, loss: 0.532207, accuracy: 0.812500\n",
      "step: 13700, loss: 0.848698, accuracy: 0.718750\n",
      "step: 13720, loss: 0.821982, accuracy: 0.640625\n",
      "step: 13740, loss: 0.786735, accuracy: 0.765625\n",
      "step: 13760, loss: 0.646711, accuracy: 0.734375\n",
      "step: 13780, loss: 0.895046, accuracy: 0.656250\n",
      "step: 13800, loss: 0.776589, accuracy: 0.781250\n",
      "step: 13820, loss: 0.923018, accuracy: 0.640625\n",
      "step: 13840, loss: 0.799641, accuracy: 0.703125\n",
      "step: 13860, loss: 0.842721, accuracy: 0.750000\n",
      "step: 13880, loss: 0.732220, accuracy: 0.812500\n",
      "step: 13900, loss: 0.812216, accuracy: 0.734375\n",
      "step: 13920, loss: 0.980663, accuracy: 0.656250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 13940, loss: 0.930254, accuracy: 0.687500\n",
      "step: 13960, loss: 0.770892, accuracy: 0.750000\n",
      "step: 13980, loss: 0.842271, accuracy: 0.703125\n",
      "step: 14000, loss: 0.821263, accuracy: 0.687500\n",
      "step: 14020, loss: 0.806356, accuracy: 0.781250\n",
      "step: 14040, loss: 0.702838, accuracy: 0.765625\n",
      "step: 14060, loss: 0.839455, accuracy: 0.656250\n",
      "step: 14080, loss: 0.810081, accuracy: 0.687500\n",
      "step: 14100, loss: 0.770759, accuracy: 0.750000\n",
      "step: 14120, loss: 0.692215, accuracy: 0.812500\n",
      "step: 14140, loss: 0.866652, accuracy: 0.718750\n",
      "step: 14160, loss: 0.733007, accuracy: 0.750000\n",
      "step: 14180, loss: 0.843458, accuracy: 0.734375\n",
      "step: 14200, loss: 1.039164, accuracy: 0.562500\n",
      "step: 14220, loss: 0.790810, accuracy: 0.750000\n",
      "step: 14240, loss: 0.610063, accuracy: 0.796875\n",
      "step: 14260, loss: 0.852136, accuracy: 0.656250\n",
      "step: 14280, loss: 0.758126, accuracy: 0.734375\n",
      "step: 14300, loss: 0.747125, accuracy: 0.750000\n",
      "step: 14320, loss: 0.769181, accuracy: 0.671875\n",
      "step: 14340, loss: 0.809205, accuracy: 0.734375\n",
      "step: 14360, loss: 0.917610, accuracy: 0.687500\n",
      "step: 14380, loss: 0.759880, accuracy: 0.656250\n",
      "step: 14400, loss: 0.742224, accuracy: 0.750000\n",
      "step: 14420, loss: 0.762259, accuracy: 0.781250\n",
      "step: 14440, loss: 0.758579, accuracy: 0.671875\n",
      "step: 14460, loss: 0.818675, accuracy: 0.718750\n",
      "step: 14480, loss: 0.638534, accuracy: 0.796875\n",
      "step: 14500, loss: 0.824196, accuracy: 0.687500\n",
      "step: 14520, loss: 0.721167, accuracy: 0.734375\n",
      "step: 14540, loss: 0.848108, accuracy: 0.703125\n",
      "step: 14560, loss: 0.812296, accuracy: 0.781250\n",
      "step: 14580, loss: 0.915963, accuracy: 0.750000\n",
      "step: 14600, loss: 0.997834, accuracy: 0.625000\n",
      "step: 14620, loss: 0.843029, accuracy: 0.734375\n",
      "step: 14640, loss: 0.697946, accuracy: 0.703125\n",
      "step: 14660, loss: 0.836741, accuracy: 0.734375\n",
      "step: 14680, loss: 0.577239, accuracy: 0.765625\n",
      "step: 14700, loss: 0.792815, accuracy: 0.703125\n",
      "step: 14720, loss: 0.624173, accuracy: 0.796875\n",
      "step: 14740, loss: 0.646677, accuracy: 0.843750\n",
      "step: 14760, loss: 0.813193, accuracy: 0.718750\n",
      "step: 14780, loss: 0.712192, accuracy: 0.812500\n",
      "step: 14800, loss: 0.791329, accuracy: 0.781250\n",
      "step: 14820, loss: 0.800138, accuracy: 0.734375\n",
      "step: 14840, loss: 0.860648, accuracy: 0.703125\n",
      "step: 14860, loss: 0.702661, accuracy: 0.687500\n",
      "step: 14880, loss: 0.516328, accuracy: 0.843750\n",
      "step: 14900, loss: 0.889918, accuracy: 0.609375\n",
      "step: 14920, loss: 0.796318, accuracy: 0.718750\n",
      "step: 14940, loss: 0.830892, accuracy: 0.703125\n",
      "step: 14960, loss: 0.879036, accuracy: 0.703125\n",
      "step: 14980, loss: 0.693868, accuracy: 0.718750\n",
      "step: 15000, loss: 0.944202, accuracy: 0.671875\n",
      "step: 15020, loss: 0.907200, accuracy: 0.703125\n",
      "step: 15040, loss: 0.902449, accuracy: 0.671875\n",
      "step: 15060, loss: 0.660012, accuracy: 0.781250\n",
      "step: 15080, loss: 0.680661, accuracy: 0.796875\n",
      "step: 15100, loss: 0.959193, accuracy: 0.703125\n",
      "step: 15120, loss: 1.040226, accuracy: 0.671875\n",
      "step: 15140, loss: 0.781581, accuracy: 0.671875\n",
      "step: 15160, loss: 0.693619, accuracy: 0.781250\n",
      "step: 15180, loss: 0.963549, accuracy: 0.687500\n",
      "step: 15200, loss: 0.802005, accuracy: 0.687500\n",
      "step: 15220, loss: 0.632950, accuracy: 0.781250\n",
      "step: 15240, loss: 0.940823, accuracy: 0.718750\n",
      "step: 15260, loss: 0.709984, accuracy: 0.765625\n",
      "step: 15280, loss: 0.880092, accuracy: 0.703125\n",
      "step: 15300, loss: 0.624870, accuracy: 0.828125\n",
      "step: 15320, loss: 0.577735, accuracy: 0.859375\n",
      "step: 15340, loss: 1.111740, accuracy: 0.640625\n",
      "step: 15360, loss: 0.819398, accuracy: 0.734375\n",
      "step: 15380, loss: 0.681824, accuracy: 0.734375\n",
      "step: 15400, loss: 0.826386, accuracy: 0.750000\n",
      "step: 15420, loss: 1.173886, accuracy: 0.609375\n",
      "step: 15440, loss: 0.902505, accuracy: 0.734375\n",
      "step: 15460, loss: 0.841223, accuracy: 0.750000\n",
      "step: 15480, loss: 0.821800, accuracy: 0.718750\n",
      "step: 15500, loss: 0.663417, accuracy: 0.718750\n",
      "step: 15520, loss: 0.814545, accuracy: 0.765625\n",
      "step: 15540, loss: 0.880937, accuracy: 0.765625\n",
      "step: 15560, loss: 0.960296, accuracy: 0.718750\n",
      "step: 15580, loss: 0.684276, accuracy: 0.781250\n",
      "step: 15600, loss: 0.682809, accuracy: 0.734375\n",
      "step: 15620, loss: 0.736649, accuracy: 0.687500\n",
      "step: 15640, loss: 0.788352, accuracy: 0.765625\n",
      "step: 15660, loss: 0.791420, accuracy: 0.750000\n",
      "step: 15680, loss: 0.635748, accuracy: 0.796875\n",
      "step: 15700, loss: 0.687172, accuracy: 0.796875\n",
      "step: 15720, loss: 0.533763, accuracy: 0.812500\n",
      "step: 15740, loss: 0.582273, accuracy: 0.812500\n",
      "step: 15760, loss: 0.655814, accuracy: 0.750000\n",
      "step: 15780, loss: 0.699715, accuracy: 0.812500\n",
      "step: 15800, loss: 0.607204, accuracy: 0.812500\n",
      "step: 15820, loss: 1.054942, accuracy: 0.656250\n",
      "step: 15840, loss: 0.788730, accuracy: 0.765625\n",
      "step: 15860, loss: 0.529660, accuracy: 0.859375\n",
      "step: 15880, loss: 0.825293, accuracy: 0.734375\n",
      "step: 15900, loss: 0.565202, accuracy: 0.828125\n",
      "step: 15920, loss: 0.888718, accuracy: 0.718750\n",
      "step: 15940, loss: 0.860366, accuracy: 0.718750\n",
      "step: 15960, loss: 0.731332, accuracy: 0.734375\n",
      "step: 15980, loss: 0.566334, accuracy: 0.781250\n",
      "step: 16000, loss: 0.808943, accuracy: 0.703125\n",
      "step: 16020, loss: 0.688376, accuracy: 0.765625\n",
      "step: 16040, loss: 0.752242, accuracy: 0.734375\n",
      "step: 16060, loss: 0.835193, accuracy: 0.687500\n",
      "step: 16080, loss: 0.757273, accuracy: 0.765625\n",
      "step: 16100, loss: 0.664996, accuracy: 0.765625\n",
      "step: 16120, loss: 0.736742, accuracy: 0.734375\n",
      "step: 16140, loss: 0.761400, accuracy: 0.687500\n",
      "step: 16160, loss: 0.718815, accuracy: 0.796875\n",
      "step: 16180, loss: 0.658785, accuracy: 0.750000\n",
      "step: 16200, loss: 0.953148, accuracy: 0.625000\n",
      "step: 16220, loss: 0.742215, accuracy: 0.765625\n",
      "step: 16240, loss: 0.922856, accuracy: 0.656250\n",
      "step: 16260, loss: 0.819943, accuracy: 0.750000\n",
      "step: 16280, loss: 0.610130, accuracy: 0.765625\n",
      "step: 16300, loss: 0.820715, accuracy: 0.718750\n",
      "step: 16320, loss: 0.570480, accuracy: 0.796875\n",
      "step: 16340, loss: 0.704253, accuracy: 0.734375\n",
      "step: 16360, loss: 0.774706, accuracy: 0.765625\n",
      "step: 16380, loss: 0.893367, accuracy: 0.687500\n",
      "step: 16400, loss: 0.720117, accuracy: 0.750000\n",
      "step: 16420, loss: 0.702217, accuracy: 0.718750\n",
      "step: 16440, loss: 0.735907, accuracy: 0.765625\n",
      "step: 16460, loss: 0.682226, accuracy: 0.765625\n",
      "step: 16480, loss: 0.789674, accuracy: 0.703125\n",
      "step: 16500, loss: 0.760645, accuracy: 0.796875\n",
      "step: 16520, loss: 0.592401, accuracy: 0.828125\n",
      "step: 16540, loss: 0.673748, accuracy: 0.718750\n",
      "step: 16560, loss: 0.751192, accuracy: 0.750000\n",
      "step: 16580, loss: 0.636835, accuracy: 0.750000\n",
      "step: 16600, loss: 0.878669, accuracy: 0.687500\n",
      "step: 16620, loss: 0.683547, accuracy: 0.750000\n",
      "step: 16640, loss: 1.169019, accuracy: 0.546875\n",
      "step: 16660, loss: 0.683304, accuracy: 0.781250\n",
      "step: 16680, loss: 0.684450, accuracy: 0.750000\n",
      "step: 16700, loss: 0.792196, accuracy: 0.734375\n",
      "step: 16720, loss: 0.801579, accuracy: 0.734375\n",
      "step: 16740, loss: 0.944421, accuracy: 0.687500\n",
      "step: 16760, loss: 0.858781, accuracy: 0.656250\n",
      "step: 16780, loss: 0.667735, accuracy: 0.781250\n",
      "step: 16800, loss: 0.728662, accuracy: 0.718750\n",
      "step: 16820, loss: 0.628123, accuracy: 0.828125\n",
      "step: 16840, loss: 0.670498, accuracy: 0.796875\n",
      "step: 16860, loss: 0.601436, accuracy: 0.765625\n",
      "step: 16880, loss: 0.748540, accuracy: 0.765625\n",
      "step: 16900, loss: 0.768665, accuracy: 0.765625\n",
      "step: 16920, loss: 0.720137, accuracy: 0.718750\n",
      "step: 16940, loss: 0.699667, accuracy: 0.796875\n",
      "step: 16960, loss: 0.691471, accuracy: 0.765625\n",
      "step: 16980, loss: 0.774531, accuracy: 0.750000\n",
      "step: 17000, loss: 0.653125, accuracy: 0.781250\n",
      "step: 17020, loss: 0.628038, accuracy: 0.718750\n",
      "step: 17040, loss: 0.738575, accuracy: 0.750000\n",
      "step: 17060, loss: 0.808382, accuracy: 0.703125\n",
      "step: 17080, loss: 0.617995, accuracy: 0.796875\n",
      "step: 17100, loss: 0.604190, accuracy: 0.796875\n",
      "step: 17120, loss: 0.521232, accuracy: 0.828125\n",
      "step: 17140, loss: 0.835768, accuracy: 0.703125\n",
      "step: 17160, loss: 0.552936, accuracy: 0.859375\n",
      "step: 17180, loss: 0.534434, accuracy: 0.828125\n",
      "step: 17200, loss: 0.909082, accuracy: 0.593750\n",
      "step: 17220, loss: 0.698063, accuracy: 0.812500\n",
      "step: 17240, loss: 0.602430, accuracy: 0.718750\n",
      "step: 17260, loss: 0.573810, accuracy: 0.828125\n",
      "step: 17280, loss: 0.651983, accuracy: 0.781250\n",
      "step: 17300, loss: 0.787520, accuracy: 0.718750\n",
      "step: 17320, loss: 0.910777, accuracy: 0.718750\n",
      "step: 17340, loss: 0.953076, accuracy: 0.656250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 17360, loss: 0.706935, accuracy: 0.765625\n",
      "step: 17380, loss: 0.868747, accuracy: 0.734375\n",
      "step: 17400, loss: 0.657353, accuracy: 0.765625\n",
      "step: 17420, loss: 0.773056, accuracy: 0.796875\n",
      "step: 17440, loss: 0.586928, accuracy: 0.859375\n",
      "step: 17460, loss: 0.873968, accuracy: 0.734375\n",
      "step: 17480, loss: 0.701657, accuracy: 0.765625\n",
      "step: 17500, loss: 0.724700, accuracy: 0.796875\n",
      "step: 17520, loss: 0.815874, accuracy: 0.703125\n",
      "step: 17540, loss: 0.807460, accuracy: 0.687500\n",
      "step: 17560, loss: 0.806705, accuracy: 0.781250\n",
      "step: 17580, loss: 0.721791, accuracy: 0.765625\n",
      "step: 17600, loss: 0.694165, accuracy: 0.828125\n",
      "step: 17620, loss: 0.693135, accuracy: 0.796875\n",
      "step: 17640, loss: 0.824873, accuracy: 0.734375\n",
      "step: 17660, loss: 0.661743, accuracy: 0.843750\n",
      "step: 17680, loss: 0.668941, accuracy: 0.781250\n",
      "step: 17700, loss: 0.447876, accuracy: 0.906250\n",
      "step: 17720, loss: 0.607572, accuracy: 0.781250\n",
      "step: 17740, loss: 0.808840, accuracy: 0.734375\n",
      "step: 17760, loss: 0.783459, accuracy: 0.734375\n",
      "step: 17780, loss: 0.692964, accuracy: 0.718750\n",
      "step: 17800, loss: 0.684421, accuracy: 0.765625\n",
      "step: 17820, loss: 0.670283, accuracy: 0.734375\n",
      "step: 17840, loss: 0.812050, accuracy: 0.750000\n",
      "step: 17860, loss: 0.501609, accuracy: 0.859375\n",
      "step: 17880, loss: 0.751743, accuracy: 0.734375\n",
      "step: 17900, loss: 0.580460, accuracy: 0.781250\n",
      "step: 17920, loss: 0.828187, accuracy: 0.671875\n",
      "step: 17940, loss: 0.786516, accuracy: 0.703125\n",
      "step: 17960, loss: 0.750597, accuracy: 0.718750\n",
      "step: 17980, loss: 0.984356, accuracy: 0.609375\n",
      "step: 18000, loss: 0.739505, accuracy: 0.750000\n",
      "step: 18020, loss: 0.656400, accuracy: 0.796875\n",
      "step: 18040, loss: 0.653373, accuracy: 0.812500\n",
      "step: 18060, loss: 0.625071, accuracy: 0.812500\n",
      "step: 18080, loss: 0.511153, accuracy: 0.875000\n",
      "step: 18100, loss: 0.576216, accuracy: 0.843750\n",
      "step: 18120, loss: 0.752268, accuracy: 0.750000\n",
      "step: 18140, loss: 0.892846, accuracy: 0.718750\n",
      "step: 18160, loss: 0.894469, accuracy: 0.703125\n",
      "step: 18180, loss: 0.690331, accuracy: 0.796875\n",
      "step: 18200, loss: 0.516968, accuracy: 0.796875\n",
      "step: 18220, loss: 0.672123, accuracy: 0.765625\n",
      "step: 18240, loss: 0.803179, accuracy: 0.703125\n",
      "step: 18260, loss: 0.687715, accuracy: 0.796875\n",
      "step: 18280, loss: 0.767748, accuracy: 0.750000\n",
      "step: 18300, loss: 0.895533, accuracy: 0.718750\n",
      "step: 18320, loss: 0.602742, accuracy: 0.796875\n",
      "step: 18340, loss: 0.659845, accuracy: 0.750000\n",
      "step: 18360, loss: 0.894678, accuracy: 0.687500\n",
      "step: 18380, loss: 0.702030, accuracy: 0.734375\n",
      "step: 18400, loss: 0.739138, accuracy: 0.687500\n",
      "step: 18420, loss: 0.694246, accuracy: 0.796875\n",
      "step: 18440, loss: 0.749393, accuracy: 0.687500\n",
      "step: 18460, loss: 0.612656, accuracy: 0.796875\n",
      "step: 18480, loss: 0.441922, accuracy: 0.796875\n",
      "step: 18500, loss: 0.870660, accuracy: 0.718750\n",
      "step: 18520, loss: 0.705603, accuracy: 0.734375\n",
      "step: 18540, loss: 0.656777, accuracy: 0.765625\n",
      "step: 18560, loss: 0.918324, accuracy: 0.718750\n",
      "step: 18580, loss: 0.774196, accuracy: 0.703125\n",
      "step: 18600, loss: 0.730768, accuracy: 0.781250\n",
      "step: 18620, loss: 0.850247, accuracy: 0.703125\n",
      "step: 18640, loss: 0.495881, accuracy: 0.843750\n",
      "step: 18660, loss: 0.789647, accuracy: 0.625000\n",
      "step: 18680, loss: 0.424281, accuracy: 0.890625\n",
      "step: 18700, loss: 0.649424, accuracy: 0.765625\n",
      "step: 18720, loss: 0.444974, accuracy: 0.843750\n",
      "step: 18740, loss: 0.656202, accuracy: 0.765625\n",
      "step: 18760, loss: 0.439435, accuracy: 0.859375\n",
      "step: 18780, loss: 0.645392, accuracy: 0.718750\n",
      "step: 18800, loss: 0.667705, accuracy: 0.734375\n",
      "step: 18820, loss: 0.743165, accuracy: 0.765625\n",
      "step: 18840, loss: 0.665061, accuracy: 0.781250\n",
      "step: 18860, loss: 0.737702, accuracy: 0.734375\n",
      "step: 18880, loss: 0.756636, accuracy: 0.765625\n",
      "step: 18900, loss: 0.677796, accuracy: 0.812500\n",
      "step: 18920, loss: 0.832764, accuracy: 0.734375\n",
      "step: 18940, loss: 0.580744, accuracy: 0.843750\n",
      "step: 18960, loss: 0.746122, accuracy: 0.750000\n",
      "step: 18980, loss: 0.660987, accuracy: 0.703125\n",
      "step: 19000, loss: 0.505439, accuracy: 0.812500\n",
      "step: 19020, loss: 0.664603, accuracy: 0.796875\n",
      "step: 19040, loss: 0.698589, accuracy: 0.750000\n",
      "step: 19060, loss: 0.730500, accuracy: 0.750000\n",
      "step: 19080, loss: 0.601688, accuracy: 0.781250\n",
      "step: 19100, loss: 0.769905, accuracy: 0.718750\n",
      "step: 19120, loss: 0.667629, accuracy: 0.796875\n",
      "step: 19140, loss: 0.675744, accuracy: 0.781250\n",
      "step: 19160, loss: 0.551231, accuracy: 0.781250\n",
      "step: 19180, loss: 0.568437, accuracy: 0.828125\n",
      "step: 19200, loss: 0.862550, accuracy: 0.703125\n",
      "step: 19220, loss: 0.649084, accuracy: 0.828125\n",
      "step: 19240, loss: 0.692427, accuracy: 0.734375\n",
      "step: 19260, loss: 0.631112, accuracy: 0.734375\n",
      "step: 19280, loss: 0.772355, accuracy: 0.765625\n",
      "step: 19300, loss: 0.628002, accuracy: 0.781250\n",
      "step: 19320, loss: 0.615280, accuracy: 0.781250\n",
      "step: 19340, loss: 0.736602, accuracy: 0.734375\n",
      "step: 19360, loss: 0.767952, accuracy: 0.765625\n",
      "step: 19380, loss: 0.890533, accuracy: 0.703125\n",
      "step: 19400, loss: 0.738237, accuracy: 0.750000\n",
      "step: 19420, loss: 0.619683, accuracy: 0.796875\n",
      "step: 19440, loss: 0.664785, accuracy: 0.828125\n",
      "step: 19460, loss: 0.998178, accuracy: 0.656250\n",
      "step: 19480, loss: 0.640450, accuracy: 0.828125\n",
      "step: 19500, loss: 0.678016, accuracy: 0.765625\n",
      "step: 19520, loss: 0.738600, accuracy: 0.734375\n",
      "step: 19540, loss: 0.567391, accuracy: 0.859375\n",
      "step: 19560, loss: 0.560070, accuracy: 0.812500\n",
      "step: 19580, loss: 0.526925, accuracy: 0.812500\n",
      "step: 19600, loss: 0.755863, accuracy: 0.718750\n",
      "step: 19620, loss: 0.760550, accuracy: 0.687500\n",
      "step: 19640, loss: 0.645194, accuracy: 0.765625\n",
      "step: 19660, loss: 0.779244, accuracy: 0.765625\n",
      "step: 19680, loss: 0.692456, accuracy: 0.765625\n",
      "step: 19700, loss: 0.977755, accuracy: 0.671875\n",
      "step: 19720, loss: 0.566006, accuracy: 0.828125\n",
      "step: 19740, loss: 0.673255, accuracy: 0.781250\n",
      "step: 19760, loss: 0.654281, accuracy: 0.750000\n",
      "step: 19780, loss: 0.708996, accuracy: 0.750000\n",
      "step: 19800, loss: 0.462604, accuracy: 0.843750\n",
      "step: 19820, loss: 0.700370, accuracy: 0.750000\n",
      "step: 19840, loss: 0.731024, accuracy: 0.734375\n",
      "step: 19860, loss: 0.733060, accuracy: 0.734375\n",
      "step: 19880, loss: 0.899182, accuracy: 0.718750\n",
      "step: 19900, loss: 0.974528, accuracy: 0.687500\n",
      "step: 19920, loss: 0.830438, accuracy: 0.812500\n",
      "step: 19940, loss: 0.599154, accuracy: 0.875000\n",
      "step: 19960, loss: 0.740960, accuracy: 0.796875\n",
      "step: 19980, loss: 0.811868, accuracy: 0.703125\n",
      "step: 20000, loss: 0.721007, accuracy: 0.796875\n"
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "    run_optimization(batch_x, batch_y)\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = conv_net(batch_x)\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Train and Validation Picture')\n",
    "plt.xlabel('Times')\n",
    "plt.ylabel('Loss value')\n",
    "plt.plot(train_loss_list, color=(1, 0, 0), label='Loss train')\n",
    "plt.plot(train_acc_list, color=(0, 0, 1), label='Accuracy train')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
